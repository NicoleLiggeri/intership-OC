# Intership OpenCitations - Nicole Liggeri

## Day 1 (5/12/2023) - 5h
- Introductory meeting to establish the main goals of the internship.
- Completed the reading of suggested articles for a general understanding of the structure of OpenCitations.
- General overview of Scimago, ERIH-PLUS, DBLP, PubMed, DOAJ.

## Day 2 (6/12/2023) - 3h
- I have created a table to update the characteristics of various journal ranks (quality, license, list of primary classification, secondary classification, etc.).
- I started by analyzing Scimago, but then I moved on to ERIH-PLUS (excellent quality, cc by, etc.).
- ERIH-PLUS connects to a long list of other journal ranks (about fifteen - VABB-SHW, SciELO ecc.) and I have superficially checked some of them. I have added all of them to the table.

## Day 3 (18/1/2024) - 6h
- I have updated the table with information related to Scimago, ERIH-PLUS, DBLP, PubMed, DOAJ. Except for Scimago, the other lists are published under CC BY or CC0.
- I copied and saved the list of ERIH-Plus.

# Day 4 and 5 (6/4/2024 - 7/4/2024) - 10h
I have delved into the topic of scientific literature classification to better define the methodology to follow in the subsequent weeks.

Now I know that there are several ways to perform journal classification:

- One can rely on content-based classifications such as the one employed in the Library of Congress (Bensman, 2007) or disciplinary databases (Eykens, Guns, & Engels, 2019). Using these classifications, the assumption is that a manual inquiry of the content of a publication (e.g., title, abstract, full-text) is the best approach to identify relevant disciplinary categories.
- Another approach is to employ a citations-based classification approach (e.g., based on direct citations or bibliographic coupling) to identify clusters of publications of distinct academic disciplines or specialties.
- Thirdly, one can use text-based algorithmic approaches or a hybrid text and citation-based approach.
- Finally, it is possible to use a more basic and pragmatic approach and rely on journal classifications. However, it's noted that journal-based methods tend to be less accurate compared to manual or automated methods applied at the article level.

Other fun facts: 

> "We argue that, just like any other classifications, disciplinary classifications are social achievements resulting from local and context-specific considerations and negotiations. [...] In the recent decade, a number of new data sources have emerged (e.g., Dimensions, Microsoft Academic, Crossref, and (earlier) Google Scholar). However, studies investigating their coverage indicate that coverage is still low for SSH, especially for publications in languages other than English. Thus, at the moment, national databases remain the data sources that offer the most comprehensive coverage for research output in SSH." - Linda Sīle. Tracing the context in disciplinary classifications

> "Moreover, there are apparently non-systematic differences between fields in the extent to which journal contents match their Scopus classifications, so there is no general rule about the types of fields in which journal classifications are reliable." - Thelwall, Mike & Pinfield, Stephen. (2023). Are Scopus journal field classifications ever misleading?

I have found some classification systems that seem to be good starting points for further investigation. 

<details>
<summary>List of articles read</summary>


- Kulczycki, E., Engels, T.C.E., Pölönen, J. et al. Publication patterns in the social sciences and humanities: evidence from eight European countries. Scientometrics 116, 463–486 (2018). https://doi.org/10.1007/s11192-018-2711-0
- Linda Sīle, Raf Guns, Frédéric Vandermoere, Gunnar Sivertsen, Tim C. E. Engels; Tracing the context in disciplinary classifications: A bibliometric pairwise comparison of five classifications of journals in the social sciences and humanities. Quantitative Science Studies 2021; 2 (1): 65–88. doi: https://doi.org/10.1162/qss_a_00110
- Lavik, Gry & Sivertsen, Gunnar. (2017). Erih Plus – Making the Ssh Visible, Searchable and Available. Procedia Computer Science. 106. 10.1016/j.procs.2017.03.035.
- Daraio, C., Glänzel, W. Grand challenges in data integration—state of the art and future perspectives: an introduction. Scientometrics 108, 391–400 (2016). https://doi.org/10.1007/s11192-016-1914-5
- Thelwall, Mike, and Stephen Pinfield. "Are Scopus journal field classifications ever misleading?." arXiv preprint arXiv:2307.15449 (2023).
- Börner K, Klavans R, Patek M, Zoss AM, Biberstine JR, Light RP, et al. (2012) Design and Update of a Classification System: The UCSD Map of Science. PLoS ONE 7(7): e39464. https://doi.org/10.1371/journal.pone.0039464
- Engels, Tim & Istenic Starcic, Andreja & Kulczycki, Emanuel & Pölönen, Janne & Sivertsen, Gunnar. (2018). Are book publications disappearing from scholarly communication in the social sciences and humanities?. Aslib Journal of Information Management. 70. 10.1108/AJIM-05-2018-0127. 
- Klavans, Richard & Boyack, Kevin. (2007). Is there a convergent structure of science? A comparison of maps using the ISI and Scopus databases. Proceedings of ISSI 2007. 
- Boyack, Kevin & Klavans, Richard & Borner, Katy. (2005). Mapping the Backbone of Science. Scientometrics. 64. 351-374. 10.1007/s11192-005-0255-6. 
- Boyack, Kevin. (2009). Using detailed maps of science to identify potential collaborations. Scientometrics. 79. 27-44. 10.1007/s11192-009-0402-6. 
- Klavans, Richard & Boyack, Kevin. (2006). Identifying a better measure of relatedness for mapping science. Journal of the American Society for Information Science and Technology. 57. 251-263. 10.1002/asi.20274. 





</details>
